import sys


def main(subject):
    subject = str(subject)
    print(subject)
    """247-DATA-crude-alignment-PhonemicStructureInWhisperSpeech.ipynb

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/17q6Tdg4R1Q43LBO8nClsTAwaTCiy4c1n

    ## Make sure to change the directory and file names below before running !!!
    """

    dirname = "/scratch/gpfs/aditis/247-encoding/whisper_gpt2_datums/"
    dirname_plot = "/scratch/gpfs/aditis/247-encoding/results/figures/"

    # load in the embeddings datums
    model = "encoder"
    layer = 4

    whisper_filename = f"_whisper-tiny.en-encoder_word_layer_0{layer}.pkl"
    base_df_filename = f"-whisper-tiny.en-encoder_word_base_df.pkl"

    # for podcast:
    # whisper_filename = f"{dirname}/777-whisper-{model}-layer_0{layer}.pkl"
    # base_df_filename = f"{dirname}/base_df.pkl"

    """## import packages

    """

    # Commented out IPython magic to ensure Python compatibility.
    import numpy as np
    import torch
    import pandas as pd
    from sklearn.decomposition import PCA
    import matplotlib.pyplot as plt

    import pickle
    from scipy.spatial.distance import pdist, squareform
    from scipy.stats import zscore
    import plotly.express as px

    from scipy.cluster.hierarchy import dendrogram, linkage
    from scipy.cluster.hierarchy import cophenet
    from scipy.spatial.distance import pdist

    # %matplotlib inline
    # %autosave 180

    import nltk

    nltk.download("punkt")
    nltk.download("averaged_perceptron_tagger")
    nltk.download("universal_tagset")

    def run_pca(pca_to, arr):
        pca = PCA(n_components=pca_to, svd_solver="auto", whiten=True)
        pca_output = pca.fit_transform(np.array(arr))

        arr_return = pca_output.tolist()

        return arr_return

    """# merge embedding and base pickle and only get words that glove has"""

    # load in data
    with open(f"{dirname}{subject}{whisper_filename}", "rb") as f:
        whisper_data = pickle.load(f)

    with open(f"{dirname}{subject}{base_df_filename}", "rb") as pfile:
        whisper_base = pd.read_pickle(pfile)

    whisper_base = whisper_base.dropna(subset=["onset", "offset"])
    whisper_base.reset_index(drop=True, inplace=True)
    whisper_df = pd.DataFrame(whisper_data)

    subjs_remaining = []  #'676', '798', '7170']

    for subj in subjs_remaining:
        with open(f"{dirname}{subj}{whisper_filename}", "rb") as f:
            temp_data_df = pickle.load(f)
        temp_data_df = pd.DataFrame(temp_data_df)

        with open(f"{dirname}{subj}{base_df_filename}", "rb") as pfile:
            temp_base_df = pd.read_pickle(pfile)
        temp_base_df = temp_base_df.dropna(subset=["onset", "offset"])
        temp_base_df.reset_index(drop=True, inplace=True)

        whisper_df = pd.concat([whisper_df, temp_data_df])
        whisper_base = pd.concat([whisper_base, temp_base_df])

    def ave_emb(datum):
        print("Averaging embeddings across tokens")

        # calculate mean embeddings
        def mean_emb(embs):
            return np.array(embs.values.tolist()).mean(axis=0).tolist()

        mean_embs = datum.groupby(["adjusted_onset", "word"], sort=False)[
            "embeddings"
        ].apply(lambda x: mean_emb(x))
        mean_embs = pd.DataFrame(mean_embs)

        # replace embeddings
        idx = (
            datum.groupby(["adjusted_onset", "word"], sort=False)[
                "token_idx"
            ].transform(min)
            == datum["token_idx"]
        )
        datum = datum[idx]
        mean_embs.set_index(datum.index, inplace=True)
        datum2 = datum.copy()  # setting copy to avoid warning
        datum2.loc[:, "embeddings"] = mean_embs.embeddings
        datum = datum2  # reassign back to datum

        return datum

    whisper_df = pd.concat([whisper_base, whisper_df], axis=1)
    whisper_avg_df = ave_emb(whisper_df)
    whisper_avg_df = whisper_avg_df.rename(columns={"index": "word_index"})

    const = 384
    emb1 = []
    emb2 = []
    emb3 = []
    emb4 = []
    for emb in whisper_avg_df["embeddings"]:
        emb1.append(emb[0 : 3 * const])
        emb2.append(emb[3 * const : 6 * const])
        emb3.append(emb[6 * const : 9 * const])
        # emb4.append(emb[9*const:12*const])

    whisper_avg_df = whisper_avg_df[whisper_avg_df["embeddings"].notna()]
    whisper_avg_df = whisper_avg_df.assign(phoneme1_emb=emb1)
    whisper_avg_df = whisper_avg_df.assign(phoneme2_emb=emb2)
    whisper_avg_df = whisper_avg_df.assign(phoneme3_emb=emb3)
    # whisper_avg_df = whisper_avg_df.assign(phoneme4_emb=emb4)

    """## create glove embedding """

    def get_vector(x, glove):
        try:
            return glove.get_vector(x)
        except KeyError:
            return None

    import gensim.downloader as api

    glove = api.load("glove-wiki-gigaword-50")
    glove_emb = whisper_avg_df.word.apply(lambda x: get_vector(x.lower(), glove))

    merged_dataset = whisper_avg_df.assign(glove=glove_emb)
    merged_dataset = merged_dataset[merged_dataset["glove"].notna()]
    merged_dataset.word = merged_dataset.word.str.lower()

    """# filter out common words"""

    merged_dataset["occurences"] = merged_dataset["word"].map(
        merged_dataset["word"].value_counts()
    )
    merged_dataset = merged_dataset[merged_dataset["occurences"] < 10]

    merged_dataset.drop_duplicates("word", inplace=True, ignore_index=True)

    """# Get the part of speech for each word

    """

    words_orig, part_of_speech = zip(
        *nltk.pos_tag(merged_dataset.word, tagset="universal")
    )
    merged_dataset = merged_dataset.assign(part_of_speech=part_of_speech)

    function_content_dict = {
        "ADP": "function",
        "CONJ": "function",
        "DET": "function",
        "PRON": "function",
        "PRT": "function",
        "ADJ": "content",
        "ADV": "content",
        "NOUN": "content",
        "NUM": "content",
        "VERB": "content",
        "X": "unknown",
    }
    function_content = merged_dataset.apply(
        lambda x: function_content_dict.get(x["part_of_speech"]), axis=1
    )

    merged_dataset = merged_dataset.assign(function_content=function_content)

    """[link text](https:// [link text](https://))# break words down into phonemes"""

    ## original categorization, including specific vowel catergorization
    # phoneset = ['AA', 'AE', 'AH', 'AO', 'AW', 'AY', 'B', 'CH', 'D', 'DH', 'EH', 'ER', 'EY', 'F' , 'G', 'HH', 'IH', 'IY', 'JH', 'K', 'L',  'M', 'N' , 'NG', 'OW', 'OY', 'P',  'R', 'S',  'SH', 'T', 'TH', 'UH', 'UW', 'V', 'W', 'Y', 'Z', 'ZH']
    # place_of_articulation   = ['low-central', 'low-front', 'mid-central', 'mid-back', 'high-back', 'high-front', 'bilabial', 'post-alveolar', 'alveolar', 'inter-dental', 'mid-front', 'mid-central', 'mid-front','alveolar','velar', 'glotal', 'high-front', 'high-front', 'post-alveolar', 'velar', 'alveolar', 'bilabial', 'alveolar', 'velar', 'high-back', 'high-front', 'bilabial', 'alveolar', 'alveolar', 'post-alveolar', 'alveolar', 'inter-dental', 'high-back', 'high-back', 'labio-dental', 'bilabial', 'palatal', 'alveolar', 'post-alveolar']
    # manner_of_articulation  = ['lax', 'lax', 'lax', 'lax', 'lax', 'tense', 'stop', 'affricate', 'stop', 'fricative', 'lax', 'tense', 'tense', 'flap', 'stop','fricative', 'lax', 'tense', 'affricate', 'stop', 'lateral-liquid', 'nasal', 'nasal', 'nasal', 'lax', 'lax', 'stop', 'retroflex-liquid', 'fricative', 'fricative', 'stop', 'fricative', 'lax', 'tense', 'fricative', 'glide', 'glide', 'fricative', 'fricative']

    phoneset_categorizations = pd.read_csv(f"{dirname}phoneset.csv")
    phoneset = phoneset_categorizations.Phoneme.values
    place_of_articulation = phoneset_categorizations.iloc[:, 1].values
    manner_of_articulation = phoneset_categorizations.iloc[:, 2].values
    voiced_or_voiceless = phoneset_categorizations.iloc[:, 3].values

    place_of_articulation_dict = dict(zip(phoneset, place_of_articulation))
    manner_of_articulation_dict = dict(zip(phoneset, manner_of_articulation))
    voiced_or_voiceless_dict = dict(zip(phoneset, voiced_or_voiceless))

    cmu_dict_filename = f"{dirname}cmudict-0.7b"
    pdict = {}
    with open(cmu_dict_filename, "r", encoding="ISO-8859-1") as f:
        for line in f.readlines():
            if not line.startswith(";;;"):
                parts = line.rstrip().split()
                word = parts[0].lower()
                phones = [phone.rstrip("012") for phone in parts[1:]]
                pdict[word] = phones

    words2phonemes = []
    words2phonemes = merged_dataset.apply(
        lambda x: pdict.get(x["word"].lower()), axis=1
    )
    # merged_dataset = merged_dataset.assign(words2phonemes=words2phonemes)

    indx = 1

    words = []
    word_index = []
    phonemes = []
    phonemic_embeddings = []
    phonemic_index = []
    manner_articulation_save = []
    place_of_articulation_save = []
    voiced_or_voiceless_save = []
    function_or_content_save = []
    part_of_speech_save = []

    for count, phonems in enumerate(words2phonemes):
        if phonems is None:
            continue
        for i in range(
            np.min([indx, len(phonems)])
        ):  # dont save more embeddings than there are phonemes in a word
            name = f"phoneme{i+1}_emb"
            phonemes.append(phonems[i])
            phonemic_embeddings.append(np.array(merged_dataset.iloc[count][name]))
            words.append(merged_dataset.iloc[count]["word"])
            word_index.append(merged_dataset.iloc[count]["word_index"])
            phonemic_index.append(i)
            manner_articulation_save.append(manner_of_articulation_dict[phonems[i]])
            place_of_articulation_save.append(place_of_articulation_dict[phonems[i]])
            voiced_or_voiceless_save.append(voiced_or_voiceless_dict[phonems[i]])
            function_or_content_save.append(
                merged_dataset.iloc[count]["function_content"]
            )
            part_of_speech_save.append(merged_dataset.iloc[count]["part_of_speech"])

    phonemic_embeddings = np.vstack(phonemic_embeddings)

    phonemic_df = pd.DataFrame(phonemic_embeddings)

    """# creating the tsne plots"""

    # make sure to have at least 36 distinct colors, otherwise the plotly plot begins reusing colors
    colors_distinct = [
        "#000000",
        "#00FF00",
        "#0000FF",
        "#FF0000",
        "#01FFFE",
        "#FFA6FE",
        "#FFDB66",
        "#006401",
        "#010067",
        "#95003A",
        "#007DB5",
        "#FF00F6",
        "#FFEEE8",
        "#774D00",
        "#90FB92",
        "#0076FF",
        "#D5FF00",
        "#FF937E",
        "#6A826C",
        "#FF029D",
        "#FE8900",
        "#7A4782",
        "#7E2DD2",
        "#85A900",
        "#FF0056",
        "#A42400",
        "#00AE7E",
        "#683D3B",
        "#BDC6FF",
        "#263400",
        "#BDD393",
        "#00B917",
        "#9E008E",
        "#001544",
        "#C28C9F",
        "#FF74A3",
        "#01D0FF",
        "#004754",
        "#E56FFE",
        "#788231",
        "#0E4CA1",
        "#91D0CB",
        "#BE9970",
        "#968AE8",
        "#BB8800",
        "#43002C",
        "#DEFF74",
        "#00FFC6",
        "#FFE502",
        "#620E00",
        "#008F9C",
        "#98FF52",
        "#7544B1",
        "#B500FF",
        "#00FF78",
        "#FF6E41",
        "#005F39",
        "#6B6882",
        "#5FAD4E",
        "#A75740",
        "#A5FFD2",
        "#FFB167",
        "#009BFF",
        "#E85EBE",
    ]

    from sklearn.manifold import TSNE
    import plotly.express as px

    features = phonemic_df

    tsne = TSNE(n_components=2, perplexity=50)
    projections = pd.DataFrame(tsne.fit_transform(features))
    projections = projections.assign(
        phonemes=phonemes,
        words=words,
        word_index=word_index,
        phonemic_index=phonemic_index,
        placeOfArtic=place_of_articulation_save,
        mannerArtic=manner_articulation_save,
        voiced_or_voiceless=voiced_or_voiceless_save,
        function_or_content=function_or_content_save,
        part_of_speech=part_of_speech_save,
    )
    projections = projections[
        projections["phonemic_index"] == 0
    ]  # comment out this line if do NOT want to look at particular phonemic index

    fig = px.scatter(
        projections,
        x=0,
        y=1,
        color=projections.mannerArtic,
        labels={"color": "mannerOfArticulation"},
        hover_data=[
            "words",
            "word_index",
            "phonemes",
            "phonemic_index",
            "placeOfArtic",
            "mannerArtic",
            "voiced_or_voiceless",
            "function_or_content",
            "part_of_speech",
        ],
        color_discrete_sequence=colors_distinct,
        title=f"{subject} whisperSpeech, colored by manner of articulation of first phoneme of word, layer {layer}",
    )
    fig.write_html(
        f"{dirname_plot}/{subject}whisperSpeech-ColoredByMannerOfArticulation_layer0{layer}.html"
    )

    features = phonemic_df

    tsne = TSNE(n_components=2, perplexity=50)
    projections = pd.DataFrame(tsne.fit_transform(features))
    projections = projections.assign(
        phonemes=phonemes,
        words=words,
        word_index=word_index,
        phonemic_index=phonemic_index,
        placeOfArtic=place_of_articulation_save,
        mannerArtic=manner_articulation_save,
        voiced_or_voiceless=voiced_or_voiceless_save,
        function_or_content=function_or_content_save,
        part_of_speech=part_of_speech_save,
    )
    projections = projections[
        projections["phonemic_index"] == 0
    ]  # comment out this line if do NOT want to look at particular phonemic index

    fig = px.scatter(
        projections,
        x=0,
        y=1,
        color=projections.placeOfArtic,
        labels={"color": "placeOfArticulation"},
        hover_data=[
            "words",
            "word_index",
            "phonemes",
            "phonemic_index",
            "placeOfArtic",
            "mannerArtic",
            "voiced_or_voiceless",
            "function_or_content",
            "part_of_speech",
        ],
        color_discrete_sequence=colors_distinct,
        title=f"{subject} whisperSpeech, colored by place of articulation of first phoneme of word, layer {layer}",
    )
    fig.write_html(
        f"{dirname_plot}/{subject}whisperSpeech-ColoredByPlaceOfArticulation_layer0{layer}.html"
    )

    features = phonemic_df

    tsne = TSNE(n_components=2, perplexity=50)
    projections = pd.DataFrame(tsne.fit_transform(features))
    projections = projections.assign(
        phonemes=phonemes,
        words=words,
        word_index=word_index,
        phonemic_index=phonemic_index,
        placeOfArtic=place_of_articulation_save,
        mannerArtic=manner_articulation_save,
        voiced_or_voiceless=voiced_or_voiceless_save,
        function_or_content=function_or_content_save,
        part_of_speech=part_of_speech_save,
    )
    projections = projections[
        projections["phonemic_index"] == 0
    ]  # comment out this line if do NOT want to look at particular phonemic index

    fig = px.scatter(
        projections,
        x=0,
        y=1,
        color=projections.phonemes,
        labels={"color": "phonemes"},
        hover_data=[
            "words",
            "word_index",
            "phonemes",
            "phonemic_index",
            "placeOfArtic",
            "mannerArtic",
            "voiced_or_voiceless",
            "function_or_content",
            "part_of_speech",
        ],
        color_discrete_sequence=colors_distinct,
        title=f"{subject} whisperSpeech, colored by phoneme of first phoneme of word, layer {layer}",
    )
    fig.write_html(
        f"{dirname_plot}/{subject}whisperSpeech-ColoredByPhonemes_layer0{layer}.html"
    )

    features = phonemic_df

    tsne = TSNE(n_components=2, perplexity=50)
    projections = pd.DataFrame(tsne.fit_transform(features))
    projections = projections.assign(
        phonemes=phonemes,
        words=words,
        word_index=word_index,
        phonemic_index=phonemic_index,
        placeOfArtic=place_of_articulation_save,
        mannerArtic=manner_articulation_save,
        voiced_or_voiceless=voiced_or_voiceless_save,
        function_or_content=function_or_content_save,
        part_of_speech=part_of_speech_save,
    )
    projections = projections[
        projections["phonemic_index"] == 0
    ]  # comment out this line if do NOT want to look at particular phonemic index

    fig = px.scatter(
        projections,
        x=0,
        y=1,
        color=projections.voiced_or_voiceless,
        labels={"color": "voiced_or_voiceless"},
        hover_data=[
            "words",
            "word_index",
            "phonemes",
            "phonemic_index",
            "placeOfArtic",
            "mannerArtic",
            "voiced_or_voiceless",
            "function_or_content",
            "part_of_speech",
        ],
        color_discrete_sequence=colors_distinct,
        title=f"{subject} whisperSpeech, colored by voiced or voiceless of phoneme, first phoneme of word, layer {layer}",
    )
    fig.write_html(
        f"{dirname_plot}/{subject}whisperSpeech-ColoredByVoiced_Or_Voiceless_layer0{layer}.html"
    )


if __name__ == "__main__":
    main(sys.argv[1])
